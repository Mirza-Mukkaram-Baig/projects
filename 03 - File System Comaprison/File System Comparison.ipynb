{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e8d80b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Comperison Test:\n",
    "- File Size\n",
    "- Speed to write\n",
    "- Speed to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c2ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevent libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd281430",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metadata\n",
    "- 10 Million Data Points\n",
    "- Fake datapoints generated through code\n",
    "- 6 Columns: Size, Age, Team, Win, Date, Probability\n",
    "\n",
    "| Column Name | Data Type |\n",
    "|-------------|-----------|\n",
    "| Category    | category  |\n",
    "| Team        | category  |\n",
    "| Age         | int16     |\n",
    "| Win         | boolean   |\n",
    "| Probability | float32   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6bfa8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(size):\n",
    "    # Create Fake Dataset\n",
    "    df = pd.DataFrame()\n",
    "    df['size'] = np.random.choice(['big','medium','small'], size)\n",
    "    df['age'] = np.random.randint(1, 50, size)\n",
    "    df['team'] = np.random.choice(['red','blue','yellow','green'], size)\n",
    "    df['win'] = np.random.choice(['yes','no'], size)\n",
    "    dates = pd.date_range('2020-01-01', '2022-12-31')\n",
    "    df['date'] = np.random.choice(dates, size)\n",
    "    df['prob'] = np.random.uniform(0, 1, size)\n",
    "    return df\n",
    "\n",
    "    # Dfining Data Types\n",
    "def set_dtypes(df):\n",
    "    df['size'] = df['size'].astype('category')\n",
    "    df['team'] = df['team'].astype('category')\n",
    "    df['age'] = df['age'].astype('int16')\n",
    "    df['win'] = df['win'].map({'yes':True, 'no': False})\n",
    "    df['prob'] = df['prob'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801ae3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pandas reading and writing to CSV file\n",
    "- File Size: 488,292 KB\n",
    "\n",
    "## Advantages\n",
    "- Human Readable: CSV files are plain text files and thus human-readable.\n",
    "- Simplicity: CSV is a straightforward file format with a simple structure that can be understood and edited in a text editor.\n",
    "\n",
    "## Disadvantage\n",
    "- No Type Information: CSV files do not contain any type or formatting information for data. This can lead to issues like the loss of leading zeros in data fields, and all data is treated as plain text.\n",
    "- Inefficient with Large Datasets: When dealing with large datasets, the lack of compression and indexing can make CSVs slow to process.\n",
    "- Not Suitable for Complex Data: CSV files are not designed to contain complex, hierarchical data structures. They work best with flat data structures and struggle with nested or multi-level data.\n",
    "- Lack of Standardization: Although CSVs are simple, they lack a definitive standard, which can lead to inconsistencies in handling them. For example, data containing commas can lead to issues if not correctly escaped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a42672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing CSV\n",
      "CPU times: total: 33 s\n",
      "Wall time: 33.3 s\n",
      "CPU times: total: 6.03 s\n",
      "Wall time: 6.09 s\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing CSV')\n",
    "df = get_dataset(10_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_csv('test.csv')\n",
    "%time df_csv = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a032e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pandas reading and writing to Pickle file\n",
    "- File Size: 166,018 KB\n",
    "\n",
    "## Advantages\n",
    "- Python Integration: Pickle is built-in with Python, so it's simple to use with no extra dependencies.\n",
    "- Flexibility: Pickle can serialize nearly any Python object\n",
    "\n",
    "## Disadvantage\n",
    "- Security: Loading a pickled file can execute arbitrary code, so it's not safe to load data that came from an untrusted source.\n",
    "- Interoperability: Pickle is Python-specific and not suitable for cross-language data interchange.\n",
    "- Backward compatibility: Pickle files may not be backward-compatible or might not work across different Python versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac0b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing Pickle\n",
      "CPU times: total: 109 ms\n",
      "Wall time: 777 ms\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 86.4 ms\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing Pickle')\n",
    "df = get_dataset(10_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_pickle('test.pickle')\n",
    "%time df_pickle = pd.read_pickle('test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81a8b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pandas reading and writing to Parquet file\n",
    "- File Size: 66,624 KB\n",
    "\n",
    "## Advantages\n",
    "- Compression: Parquet offers very efficient compression and encoding schemes.\n",
    "- Columnar Format: As a column-oriented format, it's well-suited for data analysis and queries, especially for big data workloads.\n",
    "- Big Data Ecosystem: Parquet is widely supported across the big data ecosystem, including tools like Apache Spark, Apache Arrow, etc.\n",
    "- Schema Evolution: Parquet supports schema evolution which allows the structure of tables to be modified over time without breaking older readers.\n",
    "\n",
    "## Disadvantage\n",
    "- Write Speed: Parquet is a bit slower to write than other formats due to its compression mechanism\n",
    "- Complexity: The format is complex, and therefore the reading and writing software is correspondingly complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0435382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing Parquet\n",
      "CPU times: total: 1.22 s\n",
      "Wall time: 1.04 s\n",
      "CPU times: total: 1.55 s\n",
      "Wall time: 437 ms\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing Parquet')\n",
    "df = get_dataset(10_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_parquet('test.parquet')\n",
    "%time df_parquet = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05028a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pandas reading and writing to Feather file\n",
    "- File Size: 100,123 KB\n",
    "\n",
    "## Advantages\n",
    "- Speed: Feather format is designed to be fast for reading and writing data\n",
    "- Interoperability: Provides a high degree of compatibility between Python (Pandas) and R.\n",
    "- Schema Storage: Feather stores data type information, preserving the data types.\n",
    "\n",
    "## Disadvantage\n",
    "- Size: Feather files can take up more disk space than other formats because they prioritize speed over compression.\n",
    "- Support: While popular in Pandas and R, Feather isn't as widely supported in big data systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac41639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and writing Feather\n",
      "CPU times: total: 859 ms\n",
      "Wall time: 392 ms\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 226 ms\n"
     ]
    }
   ],
   "source": [
    "print('Reading and writing Feather')\n",
    "df = get_dataset(10_000_000)\n",
    "df = set_dtypes(df)\n",
    "%time df.to_feather('test.feather')\n",
    "%time df_feather = pd.read_feather('test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5917c6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Summary\n",
    "| File System | File Size (KB) | Reading Speed (s) | Writing Speed (s) | Maximum Size |\n",
    "| ------- | ------- | ------- | ------- | ------- |\n",
    "| CSV  | 488,292  | 33.3  | 6.09 | Limited by the file system and available memory; performance issues may arise with large files (multi-GB range) |\n",
    "| Pickle  | 166,018  | 0.777  | 0.0864 | Limited by available memory; very large objects may cause issues |\n",
    "| Parquet  | 66,624  | 1.04  | 0.437 | No explicit limit; practical limit depends on the file system and the tool being used to process the data. Python libraries that handle Parquet files (like pyarrow and fastparquet) can handle very large files efficiently |\n",
    "| Feather  | 100,123  | 0.392  | 0.226 | No explicit limit; as with Parquet, the practical limit depends on the file system and the tool used to process the data. Python's feather-format library can handle very large files efficiently |\n",
    "\n",
    "### File System Recommendations:\n",
    "\n",
    "1. **CSV**: CSV is a good choice for small datasets and for scenarios where readability and simplicity are paramount. It is a universal format, meaning you can use it to move data between different programs, including non-programming tools like Excel. However, it may not be the best choice for large, complex datasets due to its lack of compression and data type preservation.\n",
    "\n",
    "2. **Pickle**: Pickle is a suitable choice when working strictly within Python, especially when you need to store complex data types or custom objects. Keep in mind that pickled files can pose a security risk if they come from untrusted sources and they are not backward-compatible between Python versions.\n",
    "\n",
    "3. **Parquet**: Parquet is an excellent choice for big data processing scenarios. It provides efficient storage through its columnar format and data compression capabilities. Parquet is widely supported in the big data ecosystem (like Hadoop and Spark), making it a good choice for interoperability within these ecosystems.\n",
    "\n",
    "4. **Feather**: Feather can be an optimal choice when you need speed for reading and writing data and interoperability between Python and R. It also preserves data types. However, be aware that Feather files may occupy more disk space than other file formats and it might not be as widely supported in big data systems.\n",
    "\n",
    "The best file system to use depends heavily on your specific use case, including the size of your dataset, the complexity of your data, the programming languages you are using, and the specific requirements of your project.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptc",
   "language": "python",
   "name": "ptc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
